{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FINE-TUNING XTTS-v2 #"]},{"cell_type":"markdown","metadata":{},"source":["This notebook will contain code for training XTTS-v2 on kaggle. However, the main point is to introduce you to the various things you should consider when fine tuning XTTS-v2. The tips/advice/guides in the official coqui [docs](https://docs.coqui.ai/en/latest/) are great, but it's not always clear which docs content pertains to which model/s.\n","\n","This notebook has been tested with its original environment, Persistence set to 'Files only' and accelerator set to GPU P100."]},{"cell_type":"markdown","metadata":{},"source":["## Creating Your Dataset:\n","\n","\n","### Dataset of this Notebook:\n","The dataset attached to this notebook comes from the freely available [M-AILABS Speech DataSet](https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/). (It is supposed to be a UK English speaker reading the novel Jane Eyre, but to my ears sounds more like a US English speaker putting on an accent.)\n","\n","### Preparing your own Data:\n","I am assuming you want to fine-tune on your own data. Load your audio files into [Audacity](https://www.audacityteam.org/), select them, and export as WAV (channels: mono, sample rate: 22050, encoding: Signed 16-bit PCM).\n","\n","Doing additional audio pre-processing in Audacity is risky. My experiments with normalising loudness between different source audio recordings ended up creating a voice that was lacking in dynamic range. Similarly, my attempts to remove noise from source recordings also removed too much of the actual voice/signal and ended up producing bad results. \n","\n","My advice (unless you are experienced with working with audio or you have a lot of time to play around) is just to listen to the audio you're thinking of using and have a quick look at the wave forms/spectrograms in Audacity. Simply discard any audio that is significantly worse quality than the rest. Examples of unpromising source audio include: constant background noise (e.g., coughing, clapping, laughter), excessive clipping in waveform view of Audacity, poor quality recording with constant whine/noise/etc. .\n","\n","### Making an LJSpeech Style Dataset:\n","The format for LJSpeech is a dir that contains two things: a metadata.csv file and a dir called 'wavs' that contains your voice recordings. Each line of the metadata.csv file includes:\n","\n","1. The name of an audio file\n","2. The text for that file. E.g., \"Jane eyre by Charlotte Bronte. Chapter 1.\"\n","3. The normalised text. E.g., \"Jane eyre by Charlotte Bronte. Chapter one.\"\n","\n","**If you are fine-tuning XTTS-v2 you don't need to worry about normalising your text, because it gets done for you automatically at training time. So your second and third columns can be identical.**\n","\n","[Here](https://github.com/zuverschenken/XTTSv2Scripts) is my github repo showing how to create an LJSpeech style dataset from 1 or more WAV files that may contain multiple speakers. (Alternatively you can try the [WhisperX](https://github.com/m-bain/whisperX) project for this task.)\n","\n","After you have finished with these, your dataset will be in the LJSpeech format and ready for use with this notebook.\n","\n","[Here](https://www.kaggle.com/code/maxbr0wn/inspect-tts-dataset/) is my kaggle notebook showing you how to check the quality of your dataset and sanitise it.\n","\n","### Note on Model Performance:\n","Some degree of repetition/mushy mouth sounds seems to be inherent to the model. Even the pre-trained voices that comes packaged with TTS suffer from this problem to a small extent. There are two ways I'm aware of to improve your performance (these are already covered in other parts of this/my other notebook, but I'm putting it here again since it's pretty important):\n","\n","1. Improve the quality of your training data. Cull problematic items. Get more training data if your dataset is really small.\n","2. The model does not generalise well to unseen sequence lengths. If you only fine-tune on 10s long audio clips and then try to produce a 1s clip at inference time, it will probably struggle. Make sure you have a good distribution of training lengths. Note that when you try to generate audio from a long text string, *this program is automatically splitting that long string of text into several shorter strings*, because the model cannot generate sequences of arbitrary length. If you are suffering from garbled/repetitious outputs, then I recommend putting some print statements in the 'split_sentence\" function in TTS.tts.layers.xtts.tokenizer. This will show you how your long text is being split up. If you see that your bad outputs are only occuring when the model is trying to generate audio for very short sequences or very long sequences, then you know what needs to be addressed. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:37:13.273547Z","iopub.status.busy":"2024-04-22T13:37:13.273194Z","iopub.status.idle":"2024-04-22T13:39:18.675155Z","shell.execute_reply":"2024-04-22T13:39:18.674199Z","shell.execute_reply.started":"2024-04-22T13:37:13.273519Z"},"trusted":true},"outputs":[],"source":["!pip install git+https://github.com/coqui-ai/TTS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:39:18.677298Z","iopub.status.busy":"2024-04-22T13:39:18.676940Z","iopub.status.idle":"2024-04-22T13:39:41.538446Z","shell.execute_reply":"2024-04-22T13:39:41.537167Z","shell.execute_reply.started":"2024-04-22T13:39:18.677260Z"},"trusted":true},"outputs":[],"source":["!pip install transformers==4.37.1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:39:41.540234Z","iopub.status.busy":"2024-04-22T13:39:41.539854Z","iopub.status.idle":"2024-04-22T13:39:41.544960Z","shell.execute_reply":"2024-04-22T13:39:41.543875Z","shell.execute_reply.started":"2024-04-22T13:39:41.540197Z"},"trusted":true},"outputs":[],"source":["###updated training zone####"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:39:41.548575Z","iopub.status.busy":"2024-04-22T13:39:41.547803Z","iopub.status.idle":"2024-04-22T13:40:04.637831Z","shell.execute_reply":"2024-04-22T13:40:04.636990Z","shell.execute_reply.started":"2024-04-22T13:39:41.548541Z"},"trusted":true},"outputs":[],"source":["from trainer import Trainer, TrainerArgs\n","#from trainer.logging.wandb_logger import WandbLogger\n","from TTS.tts.configs.shared_configs import BaseDatasetConfig\n","from TTS.tts.datasets import load_tts_samples\n","from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n","from TTS.utils.manage import ModelManager\n","\n","import sys\n","import os\n","import wandb"]},{"cell_type":"markdown","metadata":{},"source":["### Monkey Patching for wandb (!!!) ###\n","\n","XTTS-v2 uses tensorboard for logging by default. Officially wandb is supported, but it breaks things when I've used it (after a few epochs creating massive amounts of artifact files). For this reason I've monkey patched the offending method so that no artifacts are added.\n","\n","If you're not using wandb, then you don't need to do this."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.639070Z","iopub.status.busy":"2024-04-22T13:40:04.638807Z","iopub.status.idle":"2024-04-22T13:40:04.644492Z","shell.execute_reply":"2024-04-22T13:40:04.643257Z","shell.execute_reply.started":"2024-04-22T13:40:04.639046Z"},"trusted":true},"outputs":[],"source":["from trainer.logging.wandb_logger import WandbLogger"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.645857Z","iopub.status.busy":"2024-04-22T13:40:04.645604Z","iopub.status.idle":"2024-04-22T13:40:04.657271Z","shell.execute_reply":"2024-04-22T13:40:04.656478Z","shell.execute_reply.started":"2024-04-22T13:40:04.645835Z"},"trusted":true},"outputs":[],"source":["def add_artifact(self, file_or_dir, name, artifact_type, aliases=None):\n","    ###instead of adding artifact, do nothing###\n","    print(f\"========Ignoring artifact: {name} {file_or_dir}========\")\n","    return\n","\n","\n","WandbLogger.add_artifact = add_artifact"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.658528Z","iopub.status.busy":"2024-04-22T13:40:04.658246Z","iopub.status.idle":"2024-04-22T13:40:04.668590Z","shell.execute_reply":"2024-04-22T13:40:04.667805Z","shell.execute_reply.started":"2024-04-22T13:40:04.658505Z"},"trusted":true},"outputs":[],"source":["# Logging parameters\n","RUN_NAME = \"kaggletest\"\n","PROJECT_NAME = \"gore\" \n","DASHBOARD_LOGGER = \"wandb\" \n","LOGGER_URI = None"]},{"cell_type":"markdown","metadata":{},"source":["### Dir for Training Run ###\n","\n","Set the training run to store model files in the persistent /kaggle/working dir. \n","\n","Note that disk space is limited to 20GB here which will fill up quickly if you are saving more than a few checkpoints. In that case, if you must train on kaggle, you can create a dir at /kaggle/temp/ and  store the runs there. **Those files will not persist once the notebook session ends, so you will need to manually download them or copy them across to the /kaggle/working/ dir before the session ends.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.669840Z","iopub.status.busy":"2024-04-22T13:40:04.669598Z","iopub.status.idle":"2024-04-22T13:40:04.679627Z","shell.execute_reply":"2024-04-22T13:40:04.678861Z","shell.execute_reply.started":"2024-04-22T13:40:04.669818Z"},"trusted":true},"outputs":[],"source":["OUT_PATH = 'run/'\n","os.makedirs(OUT_PATH, exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["Retreive the base model files. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.680843Z","iopub.status.busy":"2024-04-22T13:40:04.680578Z","iopub.status.idle":"2024-04-22T13:40:04.692161Z","shell.execute_reply":"2024-04-22T13:40:04.691284Z","shell.execute_reply.started":"2024-04-22T13:40:04.680821Z"},"trusted":true},"outputs":[],"source":["# Define the path where XTTS v2.0.1 files will be downloaded\n","CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n","os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n","\n","# DVAE files\n","DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n","MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n","\n","# Set the path to the downloaded files\n","DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n","MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n","\n","# download DVAE files if needed\n","if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n","    print(\" > Downloading DVAE files!\")\n","    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n","\n","# Download XTTS v2.0 checkpoint if needed\n","TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n","XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n","\n","# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n","TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n","XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n","\n","# download XTTS v2.0 files if needed\n","if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n","    print(\" > Downloading XTTS v2.0 files!\")\n","    ModelManager._download_model_files(\n","        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.695937Z","iopub.status.busy":"2024-04-22T13:40:04.695640Z","iopub.status.idle":"2024-04-22T13:40:04.705694Z","shell.execute_reply":"2024-04-22T13:40:04.704806Z","shell.execute_reply.started":"2024-04-22T13:40:04.695903Z"},"trusted":true},"outputs":[],"source":["training_dir = \"/culledjane-eyre-ljspeech\""]},{"cell_type":"markdown","metadata":{},"source":["### Batch Size ###\n","\n","* BATCH_SIZE is the amount of items being loaded into VRAM/memory at once.\n","\n","* GRAD_ACCUM_STEPS is the amount of times we perform a forward pass with BATCH_SIZE amount of items before updating the parameters according to the SGD algorithm.\n","\n","So a BATCH_SIZE of 2 and GRAD_ACCUM_STEPS of 32 would give an 'effective batch size' of 64 (i.e., 64 items considered per optimisation step). \n","\n","The creators of XTTS-v2 recommend an effective batch size of 252 for proper training. (I found that reducing this to 126 gave me similar performance and faster fine-tuning, but we should probably listen to them.)\n","\n","Your BATCH_SIZE will depend on how big your dataset items are and how much VRAM you have available. Make it as large as possible while ensuring that everything fits in VRAM and then make BATCH_SIZE*GRAD_ACCUM_STEPS==252."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.707049Z","iopub.status.busy":"2024-04-22T13:40:04.706736Z","iopub.status.idle":"2024-04-22T13:40:04.717415Z","shell.execute_reply":"2024-04-22T13:40:04.716603Z","shell.execute_reply.started":"2024-04-22T13:40:04.707019Z"},"trusted":true},"outputs":[],"source":["\n","OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  \n","START_WITH_EVAL = True  \n","BATCH_SIZE = 1\n","GRAD_ACUMM_STEPS = 252\n","LANGUAGE = \"en\""]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Config ###\n","\n","**NOTE: if you completed my [Inspect TTS Dataset](https://www.kaggle.com/code/maxbr0wn/inspect-tts-dataset/) notebbok, you should use the maximum audio length from your dataset as max_wav_length and ensure the length of the reference audio you want to use is within the min-max range.**\n","\n","See the comments below for things you will want to change.\n","\n","Note that the lengths below are lengths of WAV files. So if your WAV file has a sample rate of 22050, then a a max_wav_length of 370000 is: 370000/22050 = ~16.78 seconds long.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.718942Z","iopub.status.busy":"2024-04-22T13:40:04.718616Z","iopub.status.idle":"2024-04-22T13:40:04.728192Z","shell.execute_reply":"2024-04-22T13:40:04.727353Z","shell.execute_reply.started":"2024-04-22T13:40:04.718908Z"},"trusted":true},"outputs":[],"source":["\n","model_args = GPTArgs(\n","    max_conditioning_length=143677,#the audio you will use for conditioning latents should be less than this \n","    min_conditioning_length=66150,#and more than this\n","    debug_loading_failures=True,#this will print output to console and help you find problems in your ds\n","    max_wav_length=223997,#set this to >= the longest audio in your dataset  \n","    max_text_length=200, \n","    mel_norm_file=MEL_NORM_FILE,\n","    dvae_checkpoint=DVAE_CHECKPOINT,\n","    xtts_checkpoint=XTTS_CHECKPOINT,  \n","    tokenizer_file=TOKENIZER_FILE,\n","    gpt_num_audio_tokens=1026, \n","    gpt_start_audio_token=1024,\n","    gpt_stop_audio_token=1025,\n","    gpt_use_masking_gt_prompt_approach=True,\n","    gpt_use_perceiver_resampler=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Audio Config ###\n","\n","The coqui TTS docs mention inspecting your data with the CheckSpectrograms.ipynb notebook to help decide on audio parameters. I think this is irrelevant for XTTS-v2, because it doesn't use the same audio config as some of the older coqui models and doesn't have the same parameters.\n","\n","The default is 22050 for input and 24000 for output. \n","\n","**the only reason my sample rate is lower is the dataset I chose has a sample rate of 16000. Assuming you are using your own data, sample_rate and dvae_sample_rate should match your data which should probably be 22050.** "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.729378Z","iopub.status.busy":"2024-04-22T13:40:04.729137Z","iopub.status.idle":"2024-04-22T13:40:04.742369Z","shell.execute_reply":"2024-04-22T13:40:04.741614Z","shell.execute_reply.started":"2024-04-22T13:40:04.729356Z"},"trusted":true},"outputs":[],"source":["audio_config = XttsAudioConfig(sample_rate=16000, dvae_sample_rate=16000, output_sample_rate=24000) "]},{"cell_type":"markdown","metadata":{},"source":["### Speaker Reference ###\n","\n","This is the audio file that will be used for creating the conditioning latent and speaker embedding. I think this is *not* used directly for training, but just for creating the audio outputs that are generated at checkpoints for you to review the training process.\n","\n","Choosing the right speaker reference is **VERY** important for XTTS-v2. It can completely change how your model will sound. Even two clips taken from the same recording of the same speaker can produce markedly different outputs. Unfortunately I can't provide an algorithm for selecting this. I recommend that you manually go through your dataset and select approximately 10 clips of your speaker where they are saying a full sentence with an intonation/rythm/speed/style that sounds pretty good. Then just experiment with all of them and find one you like. This is especially important at inference time.\n","\n","Note that you can give a speaker reference that 'doesn't belong' to your model. For example if you want to make a US English speaker model impersonate a UK English speaker, you can provide it with a UK speaker reference file. (A better way to acheive this impersonation effect might be to fine-tune very briefly on the voice you are 'impersonating'. This seems to work better than simply changing the embedding.)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.744217Z","iopub.status.busy":"2024-04-22T13:40:04.743536Z","iopub.status.idle":"2024-04-22T13:40:04.754070Z","shell.execute_reply":"2024-04-22T13:40:04.753172Z","shell.execute_reply.started":"2024-04-22T13:40:04.744173Z"},"trusted":true},"outputs":[],"source":["SPEAKER_REFERENCE = \"culledjane-eyre-ljspeech/wavs/jane_eyre_01_f000015.wav\""]},{"cell_type":"markdown","metadata":{},"source":["### Trainer Config ###\n","\n","How long your fine-tuning needs to run for before it is 'done' depends on many factors. A common problem when fine-tuning/training generative models is encountered here: we don't have a satisfactory algorithm for evaluating which outputs are superior. After a few epochs the decreases in loss are small and it's difficult to tell by listening to the test outputs whether things are improving or not. I personally have ended training after roughly 100,000 dataset items put through the trainer (i.e., 20 epochs with a dataset of 5,000 audio files). If you are listening to the test outputs and your model is performing well after just a few epochs, then you can finish early. Listening to test outputs will give you a better sense of how your model is training rather than just looking at loss values.\n","\n","If your target voice is a US male, then it will be faster than a female speaker with Russian accent. \n","\n","Note: you can write your own text for thte test_sentences list. Keep these the same between different runs so you can compare like with like. If you're using wandb, you can easily listen to these on their site while your model is training.\n","\n","**DISCLAIMER:** Some of the parameters of this config don't make a lot of sense to me (such as the LR scheduler milestones being greater than the total amount of steps we would expect during the fine-tuning process). Anything I don't understand, I have just left the same as it was provided by the Coqui team. \n","\n","I've put comments by some parameters below"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:04.756261Z","iopub.status.busy":"2024-04-22T13:40:04.755966Z","iopub.status.idle":"2024-04-22T13:40:16.938755Z","shell.execute_reply":"2024-04-22T13:40:16.937684Z","shell.execute_reply.started":"2024-04-22T13:40:04.756239Z"},"trusted":true},"outputs":[],"source":["config = GPTTrainerConfig(\n","    run_eval=True,\n","    epochs = 1000, # assuming you want to end training manually w/ keyboard interrupt\n","    output_path=OUT_PATH,\n","    model_args=model_args,\n","    run_name=RUN_NAME,\n","    project_name=PROJECT_NAME,\n","    run_description=\"\"\"\n","        GPT XTTS training\n","        \"\"\",\n","    dashboard_logger=DASHBOARD_LOGGER,\n","    wandb_entity=None,\n","    logger_uri=LOGGER_URI,\n","    audio=audio_config,\n","    batch_size=BATCH_SIZE,\n","    batch_group_size=48,\n","    eval_batch_size=BATCH_SIZE,\n","    num_loader_workers=8, #consider decreasing if your jupyter env is crashing or similar\n","    eval_split_max_size=256, \n","    print_step=50, \n","    plot_step=100, \n","    log_model_step=1000, \n","    save_step=9999999999, #ALREADY SAVES EVERY EPOCHMaking this high on kaggle because Output dir is limited in size. I changed this to be size of training set/2 so I would effectively have a checkpoint every half epoch \n","    save_n_checkpoints=1,#if you want to store multiple checkpoint rather than just 1, increase this\n","    save_checkpoints=False,# Making this False on kaggle because Output dir is limited\n","    print_eval=False,\n","    optimizer=\"AdamW\",\n","    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n","    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n","    lr=5e-06,  \n","    lr_scheduler=\"MultiStepLR\",\n","    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n","    test_sentences=[ \n","        {\n","            \"text\": \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n","            \"speaker_wav\": SPEAKER_REFERENCE, \n","            \"language\": LANGUAGE,\n","        },\n","        {\n","            \"text\": \"This cake is great. It's so delicious and moist.\",\n","            \"speaker_wav\": SPEAKER_REFERENCE,\n","            \"language\": LANGUAGE,\n","        },\n","        {\n","            \"text\": \"And soon, nothing more terrible, nothing more true, and specious stuff that says no rational being can fear a thing it will not feel, not seeing that this is what we fear.\",\n","            \"speaker_wav\": SPEAKER_REFERENCE,\n","            \"language\": LANGUAGE,\n","        }\n","        \n","    ],\n",") \n","\n","model = GPTTrainer.init_from_config(config)"]},{"cell_type":"markdown","metadata":{},"source":["### Load Dataset ###\n","\n","The evaluation set is 1% of the training data by default. This seems very low, but when you consider that you will probably want to evaluate performance by listening to tests rather than just comparing loss values and that you might want to make the most of your potentially small dataset, then it looks more reasonable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:16.940849Z","iopub.status.busy":"2024-04-22T13:40:16.940587Z","iopub.status.idle":"2024-04-22T13:40:17.196873Z","shell.execute_reply":"2024-04-22T13:40:17.195949Z","shell.execute_reply.started":"2024-04-22T13:40:16.940824Z"},"trusted":true},"outputs":[],"source":["dataset_config = BaseDatasetConfig(\n","    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", language=LANGUAGE, path=training_dir\n",")\n","train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True, eval_split_size=0.02)"]},{"cell_type":"markdown","metadata":{},"source":["### Train! ###\n","\n","**Note on warnings:**\n","\n","The trainer will print out warnings if it encounters items in your dataset where the text exceeds 250 chars or the length of your audio exceeds max_wav_length (It will also have problems if you have data items that are < ~0.2s long, which you won't want anyway). You should remove these from your dataset or re-think how you're creating your dataset.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T13:40:17.198384Z","iopub.status.busy":"2024-04-22T13:40:17.198097Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    TrainerArgs(\n","        restore_path=None,\n","        skip_train_epoch=False,\n","        start_with_eval=START_WITH_EVAL,\n","        grad_accum_steps=GRAD_ACUMM_STEPS,\n","    ),\n","    config,\n","    output_path=OUT_PATH,\n","    model=model,\n","    train_samples=train_samples,\n","    eval_samples=eval_samples,\n",")\n","trainer.fit()"]},{"cell_type":"markdown","metadata":{},"source":["Your fine-tuned model will be stored in /kaggle/working/run"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4853531,"sourceId":8194518,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
